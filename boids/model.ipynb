{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import pygame\n",
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data, InMemoryDataset, download_url, TemporalData\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.nn import ChebConv\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting CSV To Input For Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing Our CSV And Converting To DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>dx</th>\n",
       "      <th>dy</th>\n",
       "      <th>Boids</th>\n",
       "      <th>Simulation</th>\n",
       "      <th>Timestep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>192.032076</td>\n",
       "      <td>413.277323</td>\n",
       "      <td>-1.768924</td>\n",
       "      <td>-0.825558</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>266.236092</td>\n",
       "      <td>98.829753</td>\n",
       "      <td>1.131848</td>\n",
       "      <td>2.606990</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.612704</td>\n",
       "      <td>129.962456</td>\n",
       "      <td>-4.415965</td>\n",
       "      <td>-2.354997</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536.605412</td>\n",
       "      <td>33.303273</td>\n",
       "      <td>-3.424230</td>\n",
       "      <td>4.601926</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>679.053022</td>\n",
       "      <td>882.292871</td>\n",
       "      <td>1.094493</td>\n",
       "      <td>0.031586</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            x           y        dx        dy  Boids  Simulation  Timestep\n",
       "0  192.032076  413.277323 -1.768924 -0.825558      0           0         0\n",
       "1  266.236092   98.829753  1.131848  2.606990      1           0         0\n",
       "2   62.612704  129.962456 -4.415965 -2.354997      2           0         0\n",
       "3  536.605412   33.303273 -3.424230  4.601926      3           0         0\n",
       "4  679.053022  882.292871  1.094493  0.031586      4           0         0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_sim = '../data/simulation.csv'\n",
    "sim_df = pd.read_csv(path_to_sim)\n",
    "\n",
    "sim_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Boid_i</th>\n",
       "      <th>Boid_j</th>\n",
       "      <th>Timestep</th>\n",
       "      <th>Simulation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Boid_i  Boid_j  Timestep  Simulation\n",
       "0       0      39         0           0\n",
       "1       0      57         0           0\n",
       "2       1      32         0           0\n",
       "3       1      34         0           0\n",
       "4       1      83         0           0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_sim_edges = '../data/simulation_edges.csv'\n",
    "sim_edges_df = pd.read_csv(path_to_sim_edges)\n",
    "\n",
    "sim_edges_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA Of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting DataFrame To Data Object From Pytorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toDataGraph(sim_df, sim_edges_df, node_features_names):\n",
    "    \"\"\"\n",
    "    Converts simulation data into a PyTorch Geometric Data object.\n",
    "\n",
    "    Parameters:\n",
    "    - sim_df (DataFrame): DataFrame containing node features for a specific simulation and timestep.\n",
    "    - sim_edges_df (DataFrame): DataFrame containing edge information for the simulation.\n",
    "    - node_features_names (list of str): Names of the columns in sim_df that are node features.\n",
    "\n",
    "    Returns:\n",
    "    - Data: A PyTorch Geometric Data object representing the graph for the simulation.\n",
    "    \"\"\"\n",
    "    # Convert node features and edge information into tensors\n",
    "    node_features = torch.tensor(sim_df[node_features_names].to_numpy(), dtype=torch.float)\n",
    "    edge_index = torch.tensor(sim_edges_df[['Boid_i', 'Boid_j']].to_numpy().T, dtype=torch.long)\n",
    "    edge_attributes = torch.tensor(np.ones((sim_edges_df.shape[0], 1)), dtype=torch.float)\n",
    "\n",
    "    # Create and return the Data object\n",
    "    graph = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attributes)\n",
    "    return graph\n",
    "\n",
    "def allDataGraph(sim_df, sim_edges_df):\n",
    "    \"\"\"\n",
    "    Generates a list of PyTorch Geometric Data objects for each simulation and timestep.\n",
    "\n",
    "    Parameters:\n",
    "    - sim_df (DataFrame): DataFrame containing node features for all simulations and timesteps.\n",
    "    - sim_edges_df (DataFrame): DataFrame containing edge information for all simulations and timesteps.\n",
    "\n",
    "    Returns:\n",
    "    - list of Data: A list of PyTorch Geometric Data objects, one for each simulation and timestep.\n",
    "    \"\"\"\n",
    "    # Group the data by simulation and timestep\n",
    "    sim_gb_df = sim_df.groupby(['Timestep', 'Simulation'])\n",
    "    sim_edges_gb_df = sim_edges_df.groupby(['Timestep', 'Simulation'])\n",
    "\n",
    "    graphs = []\n",
    "    # Iterate over each group and convert to a Data object\n",
    "    for key, _ in sim_gb_df:\n",
    "        curr_sim_df = sim_gb_df.get_group(key)\n",
    "        curr_sim_edges_df = sim_edges_gb_df.get_group(key)\n",
    "        curr_graph = toDataGraph(curr_sim_df, curr_sim_edges_df, ['x', 'y', 'dx', 'dy'])\n",
    "        graphs.append(curr_graph)\n",
    "\n",
    "    return graphs\n",
    "\n",
    "# Example usage\n",
    "graphs = allDataGraph(sim_df, sim_edges_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: NEXT STEP MAKES CLASS THAT GIVEN THE SIMULATION DATAFRAME AND SIMULATION EDGES DATAFRAME CREATES A DATASET OBJECT\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sim_df, sim_edges_df):\n",
    "        super(CustomDataset).__init__()\n",
    "        self.all_graphs = allDataGraph(sim_df, sim_edges_df)\n",
    "        self.sequences = [graphs[i-5:i-1] for i in range(5, len(self.all_graphs)+1)]\n",
    "        self.labels = [graphs[i-1] for i in range(5, len(self.all_graphs)+1)]\n",
    "        self.len = len(self.labels)\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.labels[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "dataset = CustomDataset(sim_df, sim_edges_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GConvGRU(torch.nn.Module):\n",
    "    r\"\"\"An implementation of the Chebyshev Graph Convolutional Gated Recurrent Unit\n",
    "    Cell. For details see this paper: `\"Structured Sequence Modeling with Graph\n",
    "    Convolutional Recurrent Networks.\" <https://arxiv.org/abs/1612.07659>`_\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input features.\n",
    "        out_channels (int): Number of output features.\n",
    "        K (int): Chebyshev filter size :math:`K`.\n",
    "        normalization (str, optional): The normalization scheme for the graph\n",
    "            Laplacian (default: :obj:`\"sym\"`):\n",
    "\n",
    "            1. :obj:`None`: No normalization\n",
    "            :math:`\\mathbf{L} = \\mathbf{D} - \\mathbf{A}`\n",
    "\n",
    "            2. :obj:`\"sym\"`: Symmetric normalization\n",
    "            :math:`\\mathbf{L} = \\mathbf{I} - \\mathbf{D}^{-1/2} \\mathbf{A}\n",
    "            \\mathbf{D}^{-1/2}`\n",
    "\n",
    "            3. :obj:`\"rw\"`: Random-walk normalization\n",
    "            :math:`\\mathbf{L} = \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A}`\n",
    "\n",
    "            You need to pass :obj:`lambda_max` to the :meth:`forward` method of\n",
    "            this operator in case the normalization is non-symmetric.\n",
    "            :obj:`\\lambda_max` should be a :class:`torch.Tensor` of size\n",
    "            :obj:`[num_graphs]` in a mini-batch scenario and a\n",
    "            scalar/zero-dimensional tensor when operating on single graphs.\n",
    "            You can pre-compute :obj:`lambda_max` via the\n",
    "            :class:`torch_geometric.transforms.LaplacianLambdaMax` transform.\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        K: int,\n",
    "        normalization: str = \"sym\",\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super(GConvGRU, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.K = K\n",
    "        self.normalization = normalization\n",
    "        self.bias = bias\n",
    "        self._create_parameters_and_layers()\n",
    "\n",
    "    def _create_update_gate_parameters_and_layers(self):\n",
    "\n",
    "        self.conv_x_z = ChebConv(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            K=self.K,\n",
    "            normalization=self.normalization,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "\n",
    "        self.conv_h_z = ChebConv(\n",
    "            in_channels=self.out_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            K=self.K,\n",
    "            normalization=self.normalization,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "\n",
    "    def _create_reset_gate_parameters_and_layers(self):\n",
    "\n",
    "        self.conv_x_r = ChebConv(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            K=self.K,\n",
    "            normalization=self.normalization,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "\n",
    "        self.conv_h_r = ChebConv(\n",
    "            in_channels=self.out_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            K=self.K,\n",
    "            normalization=self.normalization,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "\n",
    "    def _create_candidate_state_parameters_and_layers(self):\n",
    "\n",
    "        self.conv_x_h = ChebConv(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            K=self.K,\n",
    "            normalization=self.normalization,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "\n",
    "        self.conv_h_h = ChebConv(\n",
    "            in_channels=self.out_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            K=self.K,\n",
    "            normalization=self.normalization,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "\n",
    "    def _create_parameters_and_layers(self):\n",
    "        self._create_update_gate_parameters_and_layers()\n",
    "        self._create_reset_gate_parameters_and_layers()\n",
    "        self._create_candidate_state_parameters_and_layers()\n",
    "\n",
    "    def _set_hidden_state(self, X, H):\n",
    "        if H is None:\n",
    "            H = torch.zeros(X.shape[0], self.out_channels).to(X.device)\n",
    "        return H\n",
    "\n",
    "    def _calculate_update_gate(self, X, edge_index, edge_weight, H, lambda_max):\n",
    "        Z = self.conv_x_z(X, edge_index, edge_weight, lambda_max=lambda_max)\n",
    "        Z = Z + self.conv_h_z(H, edge_index, edge_weight, lambda_max=lambda_max)\n",
    "        Z = torch.sigmoid(Z)\n",
    "        return Z\n",
    "\n",
    "    def _calculate_reset_gate(self, X, edge_index, edge_weight, H, lambda_max):\n",
    "        R = self.conv_x_r(X, edge_index, edge_weight, lambda_max=lambda_max)\n",
    "        R = R + self.conv_h_r(H, edge_index, edge_weight, lambda_max=lambda_max)\n",
    "        R = torch.sigmoid(R)\n",
    "        return R\n",
    "\n",
    "    def _calculate_candidate_state(self, X, edge_index, edge_weight, H, R, lambda_max):\n",
    "        H_tilde = self.conv_x_h(X, edge_index, edge_weight, lambda_max=lambda_max)\n",
    "        H_tilde = H_tilde + self.conv_h_h(H * R, edge_index, edge_weight, lambda_max=lambda_max)\n",
    "        H_tilde = torch.tanh(H_tilde)\n",
    "        return H_tilde\n",
    "\n",
    "    def _calculate_hidden_state(self, Z, H, H_tilde):\n",
    "        H = Z * H + (1 - Z) * H_tilde\n",
    "        return H\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        X: torch.FloatTensor,\n",
    "        edge_index: torch.LongTensor,\n",
    "        edge_weight: torch.FloatTensor = None,\n",
    "        H: torch.FloatTensor = None,\n",
    "        lambda_max: torch.Tensor = None,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Making a forward pass. If edge weights are not present the forward pass\n",
    "        defaults to an unweighted graph. If the hidden state matrix is not present\n",
    "        when the forward pass is called it is initialized with zeros.\n",
    "\n",
    "        Arg types:\n",
    "            * **X** *(PyTorch Float Tensor)* - Node features.\n",
    "            * **edge_index** *(PyTorch Long Tensor)* - Graph edge indices.\n",
    "            * **edge_weight** *(PyTorch Long Tensor, optional)* - Edge weight vector.\n",
    "            * **H** *(PyTorch Float Tensor, optional)* - Hidden state matrix for all nodes.\n",
    "            * **lambda_max** *(PyTorch Tensor, optional but mandatory if normalization is not sym)* - Largest eigenvalue of Laplacian.\n",
    "\n",
    "\n",
    "        Return types:\n",
    "            * **H** *(PyTorch Float Tensor)* - Hidden state matrix for all nodes.\n",
    "        \"\"\"\n",
    "        H = self._set_hidden_state(X, H)\n",
    "        Z = self._calculate_update_gate(X, edge_index, edge_weight, H, lambda_max)\n",
    "        R = self._calculate_reset_gate(X, edge_index, edge_weight, H, lambda_max)\n",
    "        H_tilde = self._calculate_candidate_state(X, edge_index, edge_weight, H, R, lambda_max)\n",
    "        H = self._calculate_hidden_state(Z, H, H_tilde)\n",
    "        return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, node_feature_dim, hidden_dim, recurrent_dim, output_dim, k=2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = GCNConv(node_feature_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, recurrent_dim)\n",
    "        self.reccurent = GConvGRU(recurrent_dim, output_dim, k)\n",
    "    \n",
    "    def forward(self,x,edge_index,edge_weight,H=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = torch.relu(x)\n",
    "        encoder_h = self.reccurent(x,edge_index,edge_weight,H)\n",
    "        return encoder_h\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, node_feature_dim, hidden_dim, recurrent_dim, output_dim, k=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.recurrent = GConvGRU(output_dim, recurrent_dim, k)\n",
    "        self.conv2 = GCNConv(recurrent_dim, hidden_dim)\n",
    "        self.conv1 = GCNConv(hidden_dim, node_feature_dim)\n",
    "\n",
    "    def forward(self, h, edge_index, edge_weight, H=None):\n",
    "        decoder_h = self.recurrent(h, edge_index, edge_weight, H)\n",
    "        x = torch.relu(decoder_h)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        return x, decoder_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSeqGenerator(nn.Module):\n",
    "    def __init__(self, obs_len, pred_len, \n",
    "                 node_feature_dim, \n",
    "                 encoder_hidden_dim, encoder_recurrent_dim, enconder_output_dim,\n",
    "                 decoder_hidden_dim, decoder_recurrent_dim, decoder_output_dim,\n",
    "                 device,k,\n",
    "                 noise_dim=(0, ), noise_type='gaussian', noise_mix_type='ped', \n",
    "                ):\n",
    "        \n",
    "        self.obs_len = obs_len\n",
    "        self.pred_len = pred_len\n",
    "        self.node_feature_dim = node_feature_dim\n",
    "\n",
    "        self.encoder_hidden_dim = encoder_hidden_dim\n",
    "        self.encoder_recurrent_dim = encoder_recurrent_dim\n",
    "        self.enconder_output_dim = enconder_output_dim\n",
    "\n",
    "        self.decoder_hidden_dim = decoder_hidden_dim\n",
    "        self.decoder_recurrent_dim = decoder_recurrent_dim\n",
    "        self.decoder_output_dim = decoder_output_dim\n",
    "\n",
    "        self.noise_dim = noise_dim\n",
    "        self.noise_type = noise_type\n",
    "        self.noise_mix_type = noise_mix_type\n",
    "\n",
    "        self.k=k\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "                                node_feature_dim=self.node_feature_dim,\n",
    "                                hidden_dim=self.encoder_hidden_dim,\n",
    "                                recurrent_dim=self.encoder_recurrent_dim,\n",
    "                                output_dim=self.enconder_output_dim,\n",
    "                                k=self.k\n",
    "                                )\n",
    "        \n",
    "        self.decoder = Decoder(\n",
    "                                node_feature_dim=self.node_feature_dim,\n",
    "                                hidden_dim=self.decoder_hidden_dim,\n",
    "                                recurrent_dim=self.decoder_recurrent_dim,\n",
    "                                output_dim=self.decoder_output_dim,\n",
    "                                k=self.k\n",
    "                                )\n",
    "        \n",
    "        if self.noise_dim[0] == 0:\n",
    "            self.noise_dim = None\n",
    "        else:\n",
    "            self.noise_first_dim = noise_dim[0]\n",
    "        \n",
    "        def get_noise(self, shape, noise_type):\n",
    "            if noise_type == 'gaussian':\n",
    "                return torch.randn(*shape).to(device)\n",
    "            elif noise_type == 'uniform':\n",
    "                return torch.rand(*shape).sub_(0.5).mul_(2.0).to(device)\n",
    "            raise ValueError('Unrecognized noise type \"%s\"' % noise_type)\n",
    "\n",
    "        def add_noise(self, _input, user_noise=None):\n",
    "            \"\"\"\n",
    "            Inputs:\n",
    "            - _input: Tensor of shape (_, decoder_h_dim - noise_first_dim)\n",
    "            - user_noise: Generally used for inference when you want to see\n",
    "            relation between different types of noise and outputs.\n",
    "            Outputs:\n",
    "            - decoder_h: Tensor of shape (_, decoder_h_dim)\n",
    "            Example:\n",
    "\n",
    "            Here _input.size(0) is the number of boids (weren't doing batches yet).\n",
    "            Let's say 100. Lets say self.noise_dim is '(64,)', the noise shape\n",
    "            will be (100, 64). So then we concat, (100, num_feat) with (100,64) tensor\n",
    "            along dim=1, then we get a resulting vector, (100, num_feat + 64)\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            noise_shape = (_input.size(0), ) + self.noise_dim\n",
    "\n",
    "            if user_noise is not None:\n",
    "                z_decoder = user_noise\n",
    "            else:\n",
    "                z_decoder = self.get_noise(noise_shape, self.noise_type)\n",
    "\n",
    "            decoder_h = torch.cat([_input, z_decoder], dim=1)\n",
    "\n",
    "            return decoder_h\n",
    "\n",
    "    def forward(self, seq):\n",
    "        encoder_hidden_states = []\n",
    "        prev_encoder_H = None\n",
    "\n",
    "        # First get the hidden states from encoder\n",
    "        for graph in seq:\n",
    "            curr_encoder_h = self.encoder(graph.x, graph.edge_index, graph.edge_weight, prev_encoder_H)\n",
    "            encoder_hidden_states.append(curr_encoder_h)\n",
    "            prev_encoder_H = curr_encoder_h\n",
    "            \n",
    "        # Second add noise to the hidden states from encoder to feed it to decoder\n",
    "        encoder_hidden_states = [self.add_noise(h) for h in encoder_hidden_states]\n",
    "        \n",
    "        # Third pass in noisy hidden states from encoder to decoder and get last output of decoder\n",
    "        prev_decoder_H = None\n",
    "        for i, graph in enumerate(seq):\n",
    "            x, curr_decoder_h = self.decoder(encoder_hidden_states[i], graph.x, graph.edge_index, prev_decoder_H)\n",
    "            prev_decoder_H = curr_decoder_h\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GraphSeqDiscriminator(nn.Module):\n",
    "    def __init__(self, node_feature_dim, hidden_dim, recurrent_dim, enc_output_dim,k):\n",
    "        self.node_feature_dim = node_feature_dim\n",
    "        self.encoder_hidden_dim = hidden_dim\n",
    "        self.encoder_recurrent_dim = recurrent_dim\n",
    "        self.enconder_output_dim = enc_output_dim,\n",
    "        self.k=k\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "                                node_feature_dim=self.node_feature_dim,\n",
    "                                hidden_dim=self.encoder_hidden_dim,\n",
    "                                recurrent_dim=self.encoder_recurrent_dim,\n",
    "                                output_dim=self.enconder_output_dim,\n",
    "                                k=self.k\n",
    "                                )\n",
    "        self.linear = nn.Linear(self.enconder_output_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, seq):\n",
    "        encoder_hidden_states = []\n",
    "        prev_encoder_H = None\n",
    "\n",
    "        # First get the hidden states from encoder\n",
    "        for graph in seq:\n",
    "            curr_encoder_h = self.encoder(graph.x, graph.edge_index, graph.edge_weight, prev_encoder_H)\n",
    "            encoder_hidden_states.append(curr_encoder_h)\n",
    "            prev_encoder_H = curr_encoder_h\n",
    "            \n",
    "        x = self.relu(self.linear(curr_encoder_h))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges_tensor(data, threshold):\n",
    "    \"\"\"\n",
    "    Calculates edges based on a distance threshold for a tensor where the first two columns represent 'x' and 'y' coordinates,\n",
    "    and formats them in COO (Coordinate List) format with shape [2, num_edges].\n",
    "\n",
    "    Parameters:\n",
    "    - data: A tensor where each row is a point in 2D space, with the first two columns being 'x' and 'y' coordinates.\n",
    "    - threshold: The distance threshold to consider two points as connected.\n",
    "\n",
    "    Returns:\n",
    "    - edges_coo: A tensor in COO format with shape [2, num_edges], where the first row contains the source nodes and the second row contains the target nodes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate pairwise distances only for 'x' and 'y'\n",
    "    x_y = data[:, :2]  # Extract 'x' and 'y' columns\n",
    "    distances = torch.cdist(x_y, x_y)  # Compute pairwise distances\n",
    "\n",
    "    # Identify pairs within the threshold distance\n",
    "    close_pairs = distances < threshold\n",
    "\n",
    "    # Extract indices of close pairs\n",
    "    edges = torch.nonzero(close_pairs, as_tuple=False).type(torch.long)\n",
    "\n",
    "    # Filter out upper triangle including diagonal to avoid duplicates and self-connections\n",
    "    edges_filtered = edges[edges[:, 0] < edges[:, 1]]\n",
    "\n",
    "    # Transpose to get shape [2, num_edges]\n",
    "    edges_coo = edges_filtered.t()\n",
    "\n",
    "    return edges_coo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataset, generator_model, discriminator_model, criterion_g, criterion_d, optimizer_g, optimizer_d, device):\n",
    "    '''\n",
    "    Loops through the entire dataset for training.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: The dataset to train on.\n",
    "    - criterion_g: The loss function for the generator.\n",
    "    - criterion_d: The loss function for the discriminator.\n",
    "    - optimizer_g: The optimizer for the generator.\n",
    "    - optimizer_d: The optimizer for the discriminator.\n",
    "\n",
    "    '''\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    err_d_total, err_g_total = 0, 0\n",
    "    for i, (seq, next_graph_of_seq) in tqdm(enumerate(dataset), desc='Train'):\n",
    "        # Putting sequences in device\n",
    "        seq, real_next_graph_of_seq = [graph.to(device) for graph in seq], next_graph_of_seq.to(device)\n",
    "        # Get output of generator which is just node features\n",
    "        fake_graph_node_feats = generator_model(seq)\n",
    "        # Get the edges of the fake_graph_node_feats\n",
    "        edge_index = get_edges_tensor(fake_next_graph_of_seq, threshold=75)\n",
    "        # Set up edge attributes too\n",
    "        edge_attr = torch.ones((edge_index.size(dim=1), 1))\n",
    "        # Set up a Data Object from Pytorch Geometric\n",
    "        fake_next_graph_of_seq = Data(fake_graph_node_feats, edge_index, edge_attr).to(device)\n",
    "        # Creating fake sequence and real sequence where the first couple are real and the last is either predictied or real\n",
    "        real_seq = seq\n",
    "        fake_seq = seq\n",
    "\n",
    "        real_seq.append(real_next_graph_of_seq)\n",
    "        fake_seq.append(fake_next_graph_of_seq)\n",
    "        discriminator_model(real_seq)\n",
    "\n",
    "        \"\"\"         \n",
    "        Part 1 Train Discriminator\n",
    "        1. Pass in real sequence to discriminator. Calculate loss: loss(log(D(x))) (backward pass) | loss(prob. its real, itsreal) ex. loss(0.7, 1)\n",
    "        2. Pass in fake sequence from the current generator to discriminator. Calculate loss: loss(log(1-D(G(z)))) (backward pass) | loss(prob its. real, its fake) ex. loss(0.2, 0)\n",
    "        3. Step for optimizer of discriminator \n",
    "        \"\"\"\n",
    "    \n",
    "        \"\"\"  \n",
    "        Part 2 Train Generator\n",
    "        1. Pass in fake sequence from current generator to discriminator. Calculate loss (using real labels for loss) | loss(its real, its real)\n",
    "        2. Update using backward pass and step into optimizer\n",
    "        \"\"\"\n",
    "        # Part 1 \n",
    "        discriminator_model.zero_grad()\n",
    "        output_d_real = discriminator_model(real_seq)\n",
    "        err_d_real = criterion_d(output_d_real, 1)\n",
    "        err_d_real.backward()\n",
    "\n",
    "        output_d_fake = discriminator_model(fake_seq)\n",
    "        err_d_fake = criterion_d(output_d_fake, 0)\n",
    "        err_d_fake.backward()\n",
    "\n",
    "        err_d_total += (err_d_real.item() + err_d_fake.item())\n",
    "\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Part 2\n",
    "        generator_model.zero_grad()\n",
    "        output_d_fake = discriminator_model(fake_seq)\n",
    "        err_g = criterion_g(output_d_fake, 1)\n",
    "        err_g.backward()\n",
    "\n",
    "        err_g_total += err_g\n",
    "\n",
    "        optimizer_g.step()\n",
    "\n",
    "        # For plotting\n",
    "        \n",
    "        # For user\n",
    "        if i % 32 == 0:\n",
    "            err_d_total /= 32\n",
    "            err_g_total /= 32\n",
    "            print(f'Discriminator Loss: {err_d_total} | Generator Loss: {err_g_total}')\n",
    "            d_losses.append(err_d_total)\n",
    "            g_losses.append(err_g_total)\n",
    "            err_d_total = 0\n",
    "            err_g_total = 0 \n",
    "\n",
    "    return g_losses, d_losses     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, int), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     14\u001b[0m k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m---> 17\u001b[0m Discriminator \u001b[38;5;241m=\u001b[39m \u001b[43mGraphSeqDiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_feature_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_feature_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mrecurrent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecurrent_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43menc_output_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menconder_output_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m generator_model \u001b[38;5;241m=\u001b[39m GraphSeqGenerator(obs_len\u001b[38;5;241m=\u001b[39mobs_len, \n\u001b[1;32m     25\u001b[0m                                     pred_len\u001b[38;5;241m=\u001b[39mpred_len,\n\u001b[1;32m     26\u001b[0m                                     node_feature_dim\u001b[38;5;241m=\u001b[39mnode_feature_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m                                     device\u001b[38;5;241m=\u001b[39mdevice,k\u001b[38;5;241m=\u001b[39mk\n\u001b[1;32m     34\u001b[0m                                     )\n\u001b[1;32m     35\u001b[0m optimizer_G \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(generator_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "Cell \u001b[0;32mIn[89], line 9\u001b[0m, in \u001b[0;36mGraphSeqDiscriminator.__init__\u001b[0;34m(self, node_feature_dim, hidden_dim, recurrent_dim, enc_output_dim, k)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menconder_output_dim \u001b[38;5;241m=\u001b[39m enc_output_dim,\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk\u001b[38;5;241m=\u001b[39mk\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mnode_feature_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_feature_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_hidden_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrecurrent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_recurrent_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                        \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menconder_output_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menconder_output_dim, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mReLU()\n",
      "Cell \u001b[0;32mIn[80], line 6\u001b[0m, in \u001b[0;36mEncoder.__init__\u001b[0;34m(self, node_feature_dim, hidden_dim, recurrent_dim, output_dim, k)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1 \u001b[38;5;241m=\u001b[39m GCNConv(node_feature_dim, hidden_dim)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2 \u001b[38;5;241m=\u001b[39m GCNConv(hidden_dim, recurrent_dim)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreccurent \u001b[38;5;241m=\u001b[39m \u001b[43mGConvGRU\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecurrent_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[79], line 49\u001b[0m, in \u001b[0;36mGConvGRU.__init__\u001b[0;34m(self, in_channels, out_channels, K, normalization, bias)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalization \u001b[38;5;241m=\u001b[39m normalization\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m bias\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_parameters_and_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[79], line 106\u001b[0m, in \u001b[0;36mGConvGRU._create_parameters_and_layers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_parameters_and_layers\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_update_gate_parameters_and_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_reset_gate_parameters_and_layers()\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_candidate_state_parameters_and_layers()\n",
      "Cell \u001b[0;32mIn[79], line 53\u001b[0m, in \u001b[0;36mGConvGRU._create_update_gate_parameters_and_layers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_update_gate_parameters_and_layers\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_x_z \u001b[38;5;241m=\u001b[39m \u001b[43mChebConv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_h_z \u001b[38;5;241m=\u001b[39m ChebConv(\n\u001b[1;32m     62\u001b[0m         in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels,\n\u001b[1;32m     63\u001b[0m         out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m         bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m     67\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aiml2/lib/python3.11/site-packages/torch_geometric/nn/conv/cheb_conv.py:91\u001b[0m, in \u001b[0;36mChebConv.__init__\u001b[0;34m(self, in_channels, out_channels, K, normalization, bias, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels \u001b[38;5;241m=\u001b[39m out_channels\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalization \u001b[38;5;241m=\u001b[39m normalization\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlins \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList(\u001b[43m[\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_initializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mglorot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(Tensor(out_channels))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aiml2/lib/python3.11/site-packages/torch_geometric/nn/conv/cheb_conv.py:92\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels \u001b[38;5;241m=\u001b[39m out_channels\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalization \u001b[38;5;241m=\u001b[39m normalization\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlins \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[0;32m---> 92\u001b[0m     \u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_initializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mglorot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(K)\n\u001b[1;32m     94\u001b[0m ])\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(Tensor(out_channels))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aiml2/lib/python3.11/site-packages/torch_geometric/nn/dense/linear.py:104\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_channels, out_channels, bias, weight_initializer, bias_initializer)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_initializer \u001b[38;5;241m=\u001b[39m bias_initializer\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_channels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mparameter\u001b[38;5;241m.\u001b[39mUninitializedParameter()\n",
      "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, int), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "obs_len = 4\n",
    "pred_len= 5\n",
    "node_feature_dim = 4\n",
    "encoder_hidden_dim =32\n",
    "encoder_recurrent_dim =32\n",
    "enconder_output_dim = 32\n",
    "decoder_hidden_dim = 32\n",
    "decoder_recurrent_dim =32\n",
    "decoder_output_dim =4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hidden_dim = 32\n",
    "recurrent_dim = 32\n",
    "output_dim = 4\n",
    "k=2\n",
    "\n",
    "\n",
    "Discriminator = GraphSeqDiscriminator(node_feature_dim=node_feature_dim, \n",
    "                                      hidden_dim=hidden_dim, \n",
    "                                      recurrent_dim=recurrent_dim, \n",
    "                                      enc_output_dim=enconder_output_dim, \n",
    "                                      k=k\n",
    "                                      )\n",
    "\n",
    "generator_model = GraphSeqGenerator(obs_len=obs_len, \n",
    "                                    pred_len=pred_len,\n",
    "                                    node_feature_dim=node_feature_dim,\n",
    "                                    encoder_hidden_dim=encoder_hidden_dim, \n",
    "                                    encoder_recurrent_dim=encoder_recurrent_dim, \n",
    "                                    enconder_output_dim=enconder_output_dim,\n",
    "                                    decoder_hidden_dim=decoder_hidden_dim, \n",
    "                                    decoder_recurrent_dim=decoder_recurrent_dim, \n",
    "                                    decoder_output_dim=decoder_output_dim,\n",
    "                                    device=device,k=k\n",
    "                                    )\n",
    "optimizer_G = optim.Adam(generator_model.parameters(), lr=0.001)\n",
    "optimizer_D = optim.Adam(Discriminator.parameters(), lr=0.001)\n",
    "criterion_g = nn.BCELoss()\n",
    "criterion_d = nn.BCELoss()\n",
    "Discriminator.train()\n",
    "generator_model.train()\n",
    "epochs = 100\n",
    "for i in range(epochs):\n",
    "    train_loop(dataset,generator_model,Discriminator,criterion_g,criterion_d,\n",
    "               optimizer_G,optimizer_D,device\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Version 2 easier to read?\n",
    "\n",
    "# def train_generator(generator_model, discriminator_model, criterion_g, optimizer_g, fake_seq):\n",
    "#     \"\"\"Train the generator model.\"\"\"\n",
    "#     generator_model.zero_grad()\n",
    "#     output_d_fake = discriminator_model(fake_seq)\n",
    "#     err_g = criterion_g(output_d_fake, torch.ones_like(output_d_fake))\n",
    "#     err_g.backward()\n",
    "#     optimizer_g.step()\n",
    "#     return err_g.item()\n",
    "\n",
    "# def train_discriminator(discriminator_model, criterion_d, optimizer_d, real_seq, fake_seq):\n",
    "#     \"\"\"Train the discriminator model.\"\"\"\n",
    "#     discriminator_model.zero_grad()\n",
    "\n",
    "#     # Real sequence\n",
    "#     output_d_real = discriminator_model(real_seq)\n",
    "#     err_d_real = criterion_d(output_d_real, torch.ones_like(output_d_real))\n",
    "#     err_d_real.backward()\n",
    "\n",
    "#     # Fake sequence\n",
    "#     output_d_fake = discriminator_model(fake_seq)\n",
    "#     err_d_fake = criterion_d(output_d_fake, torch.zeros_like(output_d_fake))\n",
    "#     err_d_fake.backward()\n",
    "\n",
    "#     optimizer_d.step()\n",
    "#     err_d_total = err_d_real.item() + err_d_fake.item()\n",
    "#     return err_d_total\n",
    "\n",
    "# def create_fake_sequence(generator_model, seq, device):\n",
    "#     \"\"\"Generate a fake sequence using the generator model.\"\"\"\n",
    "#     fake_graph_node_feats = generator_model(seq)\n",
    "#     edge_index = get_edges_tensor(fake_graph_node_feats, threshold=75)\n",
    "#     edge_attr = torch.ones((edge_index.size(dim=1), 1))\n",
    "#     fake_next_graph_of_seq = Data(fake_graph_node_feats, edge_index, edge_attr).to(device)\n",
    "#     fake_seq = seq + [fake_next_graph_of_seq]\n",
    "#     return fake_seq\n",
    "\n",
    "# def prepare_sequences(seq, next_graph_of_seq, device):\n",
    "#     \"\"\"Prepare real and fake sequences for training.\"\"\"\n",
    "#     seq = [graph.to(device) for graph in seq]\n",
    "#     real_next_graph_of_seq = next_graph_of_seq.to(device)\n",
    "#     real_seq = seq + [real_next_graph_of_seq]\n",
    "#     return seq, real_seq\n",
    "\n",
    "# def train_loop(dataset, generator_model, discriminator_model, criterion_g, criterion_d, optimizer_g, optimizer_d, device):\n",
    "#     \"\"\"Loop through the entire dataset for training.\"\"\"\n",
    "#     g_losses = []\n",
    "#     d_losses = []\n",
    "#     err_d_total, err_g_total = 0, 0\n",
    "\n",
    "#     for i, (seq, next_graph_of_seq) in tqdm(enumerate(dataset), desc='Train'):\n",
    "#         seq, real_seq = prepare_sequences(seq, next_graph_of_seq, device)\n",
    "#         fake_seq = create_fake_sequence(generator_model, seq, device)\n",
    "\n",
    "#         # Train discriminator\n",
    "#         err_d_total += train_discriminator(discriminator_model, criterion_d, optimizer_d, real_seq, fake_seq)\n",
    "\n",
    "#         # Train generator\n",
    "#         err_g_total += train_generator(generator_model, discriminator_model, criterion_g, optimizer_g, fake_seq)\n",
    "\n",
    "#         # Log losses for user\n",
    "#         if (i + 1) % 32 == 0:\n",
    "#             avg_err_d = err_d_total / 32\n",
    "#             avg_err_g = err_g_total / 32\n",
    "#             print(f'Iteration {i+1}, Discriminator Loss: {avg_err_d:.4f}, Generator Loss: {avg_err_g:.4f}')\n",
    "#             d_losses.append(avg_err_d)\n",
    "#             g_losses.append(avg_err_g)\n",
    "#             err_d_total, err_g_total = 0, 0\n",
    "\n",
    "#     return g_losses, d_losses\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
