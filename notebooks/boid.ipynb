{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Birds and Things of that Nature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def make_mlp(dim_list, activation='relu', batch_norm=True, dropout=0):\n",
    "    layers = []\n",
    "    for dim_in, dim_out in zip(dim_list[:-1], dim_list[1:]):\n",
    "        layers.append(nn.Linear(dim_in, dim_out))\n",
    "        if batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(dim_out))\n",
    "        if activation == 'relu':\n",
    "            layers.append(nn.ReLU())\n",
    "        elif activation == 'leakyrelu':\n",
    "            layers.append(nn.LeakyReLU())\n",
    "        if dropout > 0:\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def get_noise(shape, noise_type):\n",
    "    if noise_type == 'gaussian':\n",
    "        return torch.randn(*shape).cuda()\n",
    "    elif noise_type == 'uniform':\n",
    "        return torch.rand(*shape).sub_(0.5).mul_(2.0).cuda()\n",
    "    raise ValueError('Unrecognized noise type \"%s\"' % noise_type)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder is part of both TrajectoryGenerator and\n",
    "    TrajectoryDiscriminator\"\"\"\n",
    "    def __init__(\n",
    "        self, embedding_dim=64, h_dim=64, mlp_dim=1024, num_layers=1,\n",
    "        dropout=0.0\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.mlp_dim = 1024\n",
    "        self.h_dim = h_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.encoder = nn.LSTM(\n",
    "            embedding_dim, h_dim, num_layers, dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.spatial_embedding = nn.Linear(2, embedding_dim)\n",
    "\n",
    "    def init_hidden(self, batch):\n",
    "        return (\n",
    "            torch.zeros(self.num_layers, batch, self.h_dim).cuda(),\n",
    "            torch.zeros(self.num_layers, batch, self.h_dim).cuda()\n",
    "        )\n",
    "\n",
    "    def forward(self, obs_traj):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - obs_traj: Tensor of shape (obs_len, batch, 2)\n",
    "        Output:\n",
    "        - final_h: Tensor of shape (self.num_layers, batch, self.h_dim)\n",
    "        \"\"\"\n",
    "        # Encode observed Trajectory\n",
    "        batch = obs_traj.size(1)\n",
    "        obs_traj_embedding = self.spatial_embedding(obs_traj.view(-1, 2))\n",
    "        obs_traj_embedding = obs_traj_embedding.view(\n",
    "            -1, batch, self.embedding_dim\n",
    "        )\n",
    "        state_tuple = self.init_hidden(batch)\n",
    "        output, state = self.encoder(obs_traj_embedding, state_tuple)\n",
    "        final_h = state[0]\n",
    "        return final_h\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder is part of TrajectoryGenerator\"\"\"\n",
    "    def __init__(\n",
    "        self, seq_len, embedding_dim=64, h_dim=128, mlp_dim=1024, num_layers=1,\n",
    "        pool_every_timestep=True, dropout=0.0, bottleneck_dim=1024,\n",
    "        activation='relu', batch_norm=True, pooling_type='pool_net',\n",
    "        neighborhood_size=2.0, grid_size=8\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.pool_every_timestep = pool_every_timestep\n",
    "\n",
    "        self.decoder = nn.LSTM(\n",
    "            embedding_dim, h_dim, num_layers, dropout=dropout\n",
    "        )\n",
    "\n",
    "        if pool_every_timestep:\n",
    "            if pooling_type == 'pool_net':\n",
    "                self.pool_net = PoolHiddenNet(\n",
    "                    embedding_dim=self.embedding_dim,\n",
    "                    h_dim=self.h_dim,\n",
    "                    mlp_dim=mlp_dim,\n",
    "                    bottleneck_dim=bottleneck_dim,\n",
    "                    activation=activation,\n",
    "                    batch_norm=batch_norm,\n",
    "                    dropout=dropout\n",
    "                )\n",
    "            elif pooling_type == 'spool':\n",
    "                self.pool_net = SocialPooling(\n",
    "                    h_dim=self.h_dim,\n",
    "                    activation=activation,\n",
    "                    batch_norm=batch_norm,\n",
    "                    dropout=dropout,\n",
    "                    neighborhood_size=neighborhood_size,\n",
    "                    grid_size=grid_size\n",
    "                )\n",
    "\n",
    "            mlp_dims = [h_dim + bottleneck_dim, mlp_dim, h_dim]\n",
    "            self.mlp = make_mlp(\n",
    "                mlp_dims,\n",
    "                activation=activation,\n",
    "                batch_norm=batch_norm,\n",
    "                dropout=dropout\n",
    "            )\n",
    "\n",
    "        self.spatial_embedding = nn.Linear(2, embedding_dim)\n",
    "        self.hidden2pos = nn.Linear(h_dim, 2)\n",
    "\n",
    "    def forward(self, last_pos, last_pos_rel, state_tuple, seq_start_end):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - last_pos: Tensor of shape (batch, 2)\n",
    "        - last_pos_rel: Tensor of shape (batch, 2)\n",
    "        - state_tuple: (hh, ch) each tensor of shape (num_layers, batch, h_dim)\n",
    "        - seq_start_end: A list of tuples which delimit sequences within batch\n",
    "        Output:\n",
    "        - pred_traj: tensor of shape (self.seq_len, batch, 2)\n",
    "        \"\"\"\n",
    "        batch = last_pos.size(0)\n",
    "        pred_traj_fake_rel = []\n",
    "        decoder_input = self.spatial_embedding(last_pos_rel)\n",
    "        decoder_input = decoder_input.view(1, batch, self.embedding_dim)\n",
    "\n",
    "        for _ in range(self.seq_len):\n",
    "            output, state_tuple = self.decoder(decoder_input, state_tuple)\n",
    "            rel_pos = self.hidden2pos(output.view(-1, self.h_dim))\n",
    "            curr_pos = rel_pos + last_pos\n",
    "\n",
    "            if self.pool_every_timestep:\n",
    "                decoder_h = state_tuple[0]\n",
    "                pool_h = self.pool_net(decoder_h, seq_start_end, curr_pos)\n",
    "                decoder_h = torch.cat(\n",
    "                    [decoder_h.view(-1, self.h_dim), pool_h], dim=1)\n",
    "                decoder_h = self.mlp(decoder_h)\n",
    "                decoder_h = torch.unsqueeze(decoder_h, 0)\n",
    "                state_tuple = (decoder_h, state_tuple[1])\n",
    "\n",
    "            embedding_input = rel_pos\n",
    "\n",
    "            decoder_input = self.spatial_embedding(embedding_input)\n",
    "            decoder_input = decoder_input.view(1, batch, self.embedding_dim)\n",
    "            pred_traj_fake_rel.append(rel_pos.view(batch, -1))\n",
    "            last_pos = curr_pos\n",
    "\n",
    "        pred_traj_fake_rel = torch.stack(pred_traj_fake_rel, dim=0)\n",
    "        return pred_traj_fake_rel, state_tuple[0]\n",
    "\n",
    "\n",
    "class PoolHiddenNet(nn.Module):\n",
    "    \"\"\"Pooling module as proposed in our paper\"\"\"\n",
    "    def __init__(\n",
    "        self, embedding_dim=64, h_dim=64, mlp_dim=1024, bottleneck_dim=1024,\n",
    "        activation='relu', batch_norm=True, dropout=0.0\n",
    "    ):\n",
    "        super(PoolHiddenNet, self).__init__()\n",
    "\n",
    "        self.mlp_dim = 1024\n",
    "        self.h_dim = h_dim\n",
    "        self.bottleneck_dim = bottleneck_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        mlp_pre_dim = embedding_dim + h_dim\n",
    "        mlp_pre_pool_dims = [mlp_pre_dim, 512, bottleneck_dim]\n",
    "\n",
    "        self.spatial_embedding = nn.Linear(2, embedding_dim)\n",
    "        self.mlp_pre_pool = make_mlp(\n",
    "            mlp_pre_pool_dims,\n",
    "            activation=activation,\n",
    "            batch_norm=batch_norm,\n",
    "            dropout=dropout)\n",
    "\n",
    "    def repeat(self, tensor, num_reps):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        -tensor: 2D tensor of any shape\n",
    "        -num_reps: Number of times to repeat each row\n",
    "        Outpus:\n",
    "        -repeat_tensor: Repeat each row such that: R1, R1, R2, R2\n",
    "        \"\"\"\n",
    "        col_len = tensor.size(1)\n",
    "        tensor = tensor.unsqueeze(dim=1).repeat(1, num_reps, 1)\n",
    "        tensor = tensor.view(-1, col_len)\n",
    "        return tensor\n",
    "\n",
    "    def forward(self, h_states, seq_start_end, end_pos):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - h_states: Tensor of shape (num_layers, batch, h_dim)\n",
    "        - seq_start_end: A list of tuples which delimit sequences within batch\n",
    "        - end_pos: Tensor of shape (batch, 2)\n",
    "        Output:\n",
    "        - pool_h: Tensor of shape (batch, bottleneck_dim)\n",
    "        \"\"\"\n",
    "        pool_h = []\n",
    "        for _, (start, end) in enumerate(seq_start_end):\n",
    "            start = start.item()\n",
    "            end = end.item()\n",
    "            num_ped = end - start\n",
    "            curr_hidden = h_states.view(-1, self.h_dim)[start:end]\n",
    "            curr_end_pos = end_pos[start:end]\n",
    "            # Repeat -> H1, H2, H1, H2\n",
    "            curr_hidden_1 = curr_hidden.repeat(num_ped, 1)\n",
    "            # Repeat position -> P1, P2, P1, P2\n",
    "            curr_end_pos_1 = curr_end_pos.repeat(num_ped, 1)\n",
    "            # Repeat position -> P1, P1, P2, P2\n",
    "            curr_end_pos_2 = self.repeat(curr_end_pos, num_ped)\n",
    "            curr_rel_pos = curr_end_pos_1 - curr_end_pos_2\n",
    "            curr_rel_embedding = self.spatial_embedding(curr_rel_pos)\n",
    "            mlp_h_input = torch.cat([curr_rel_embedding, curr_hidden_1], dim=1)\n",
    "            curr_pool_h = self.mlp_pre_pool(mlp_h_input)\n",
    "            curr_pool_h = curr_pool_h.view(num_ped, num_ped, -1).max(1)[0]\n",
    "            pool_h.append(curr_pool_h)\n",
    "        pool_h = torch.cat(pool_h, dim=0)\n",
    "        return pool_h\n",
    "\n",
    "\n",
    "class SocialPooling(nn.Module):\n",
    "    \"\"\"Current state of the art pooling mechanism:\n",
    "    http://cvgl.stanford.edu/papers/CVPR16_Social_LSTM.pdf\"\"\"\n",
    "    def __init__(\n",
    "        self, h_dim=64, activation='relu', batch_norm=True, dropout=0.0,\n",
    "        neighborhood_size=2.0, grid_size=8, pool_dim=None\n",
    "    ):\n",
    "        super(SocialPooling, self).__init__()\n",
    "        self.h_dim = h_dim\n",
    "        self.grid_size = grid_size\n",
    "        self.neighborhood_size = neighborhood_size\n",
    "        if pool_dim:\n",
    "            mlp_pool_dims = [grid_size * grid_size * h_dim, pool_dim]\n",
    "        else:\n",
    "            mlp_pool_dims = [grid_size * grid_size * h_dim, h_dim]\n",
    "\n",
    "        self.mlp_pool = make_mlp(\n",
    "            mlp_pool_dims,\n",
    "            activation=activation,\n",
    "            batch_norm=batch_norm,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "    def get_bounds(self, ped_pos):\n",
    "        top_left_x = ped_pos[:, 0] - self.neighborhood_size / 2\n",
    "        top_left_y = ped_pos[:, 1] + self.neighborhood_size / 2\n",
    "        bottom_right_x = ped_pos[:, 0] + self.neighborhood_size / 2\n",
    "        bottom_right_y = ped_pos[:, 1] - self.neighborhood_size / 2\n",
    "        top_left = torch.stack([top_left_x, top_left_y], dim=1)\n",
    "        bottom_right = torch.stack([bottom_right_x, bottom_right_y], dim=1)\n",
    "        return top_left, bottom_right\n",
    "\n",
    "    def get_grid_locations(self, top_left, other_pos):\n",
    "        cell_x = torch.floor(\n",
    "            ((other_pos[:, 0] - top_left[:, 0]) / self.neighborhood_size) *\n",
    "            self.grid_size)\n",
    "        cell_y = torch.floor(\n",
    "            ((top_left[:, 1] - other_pos[:, 1]) / self.neighborhood_size) *\n",
    "            self.grid_size)\n",
    "        grid_pos = cell_x + cell_y * self.grid_size\n",
    "        return grid_pos\n",
    "\n",
    "    def repeat(self, tensor, num_reps):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        -tensor: 2D tensor of any shape\n",
    "        -num_reps: Number of times to repeat each row\n",
    "        Outpus:\n",
    "        -repeat_tensor: Repeat each row such that: R1, R1, R2, R2\n",
    "        \"\"\"\n",
    "        col_len = tensor.size(1)\n",
    "        tensor = tensor.unsqueeze(dim=1).repeat(1, num_reps, 1)\n",
    "        tensor = tensor.view(-1, col_len)\n",
    "        return tensor\n",
    "\n",
    "    def forward(self, h_states, seq_start_end, end_pos):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - h_states: Tesnsor of shape (num_layers, batch, h_dim)\n",
    "        - seq_start_end: A list of tuples which delimit sequences within batch.\n",
    "        - end_pos: Absolute end position of obs_traj (batch, 2)\n",
    "        Output:\n",
    "        - pool_h: Tensor of shape (batch, h_dim)\n",
    "        \"\"\"\n",
    "        pool_h = []\n",
    "        for _, (start, end) in enumerate(seq_start_end):\n",
    "            start = start.item()\n",
    "            end = end.item()\n",
    "            num_ped = end - start\n",
    "            grid_size = self.grid_size * self.grid_size\n",
    "            curr_hidden = h_states.view(-1, self.h_dim)[start:end]\n",
    "            curr_hidden_repeat = curr_hidden.repeat(num_ped, 1)\n",
    "            curr_end_pos = end_pos[start:end]\n",
    "            curr_pool_h_size = (num_ped * grid_size) + 1\n",
    "            curr_pool_h = curr_hidden.new_zeros((curr_pool_h_size, self.h_dim))\n",
    "            # curr_end_pos = curr_end_pos.data\n",
    "            top_left, bottom_right = self.get_bounds(curr_end_pos)\n",
    "\n",
    "            # Repeat position -> P1, P2, P1, P2\n",
    "            curr_end_pos = curr_end_pos.repeat(num_ped, 1)\n",
    "            # Repeat bounds -> B1, B1, B2, B2\n",
    "            top_left = self.repeat(top_left, num_ped)\n",
    "            bottom_right = self.repeat(bottom_right, num_ped)\n",
    "\n",
    "            grid_pos = self.get_grid_locations(\n",
    "                    top_left, curr_end_pos).type_as(seq_start_end)\n",
    "            # Make all positions to exclude as non-zero\n",
    "            # Find which peds to exclude\n",
    "            x_bound = ((curr_end_pos[:, 0] >= bottom_right[:, 0]) +\n",
    "                       (curr_end_pos[:, 0] <= top_left[:, 0]))\n",
    "            y_bound = ((curr_end_pos[:, 1] >= top_left[:, 1]) +\n",
    "                       (curr_end_pos[:, 1] <= bottom_right[:, 1]))\n",
    "\n",
    "            within_bound = x_bound + y_bound\n",
    "            within_bound[0::num_ped + 1] = 1  # Don't include the ped itself\n",
    "            within_bound = within_bound.view(-1)\n",
    "\n",
    "            # This is a tricky way to get scatter add to work. Helps me avoid a\n",
    "            # for loop. Offset everything by 1. Use the initial 0 position to\n",
    "            # dump all uncessary adds.\n",
    "            grid_pos += 1\n",
    "            total_grid_size = self.grid_size * self.grid_size\n",
    "            offset = torch.arange(\n",
    "                0, total_grid_size * num_ped, total_grid_size\n",
    "            ).type_as(seq_start_end)\n",
    "\n",
    "            offset = self.repeat(offset.view(-1, 1), num_ped).view(-1)\n",
    "            grid_pos += offset\n",
    "            grid_pos[within_bound != 0] = 0\n",
    "            grid_pos = grid_pos.view(-1, 1).expand_as(curr_hidden_repeat)\n",
    "\n",
    "            curr_pool_h = curr_pool_h.scatter_add(0, grid_pos,\n",
    "                                                  curr_hidden_repeat)\n",
    "            curr_pool_h = curr_pool_h[1:]\n",
    "            pool_h.append(curr_pool_h.view(num_ped, -1))\n",
    "\n",
    "        pool_h = torch.cat(pool_h, dim=0)\n",
    "        pool_h = self.mlp_pool(pool_h)\n",
    "        return pool_h\n",
    "\n",
    "\n",
    "class TrajectoryGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self, obs_len, pred_len, embedding_dim=64, encoder_h_dim=64,\n",
    "        decoder_h_dim=128, mlp_dim=1024, num_layers=1, noise_dim=(0, ),\n",
    "        noise_type='gaussian', noise_mix_type='ped', pooling_type=None,\n",
    "        pool_every_timestep=True, dropout=0.0, bottleneck_dim=1024,\n",
    "        activation='relu', batch_norm=True, neighborhood_size=2.0, grid_size=8\n",
    "    ):\n",
    "        super(TrajectoryGenerator, self).__init__()\n",
    "\n",
    "        if pooling_type and pooling_type.lower() == 'none':\n",
    "            pooling_type = None\n",
    "\n",
    "        self.obs_len = obs_len\n",
    "        self.pred_len = pred_len\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.encoder_h_dim = encoder_h_dim\n",
    "        self.decoder_h_dim = decoder_h_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.noise_dim = noise_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.noise_type = noise_type\n",
    "        self.noise_mix_type = noise_mix_type\n",
    "        self.pooling_type = pooling_type\n",
    "        self.noise_first_dim = 0\n",
    "        self.pool_every_timestep = pool_every_timestep\n",
    "        self.bottleneck_dim = 1024\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            embedding_dim=embedding_dim,\n",
    "            h_dim=encoder_h_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            pred_len,\n",
    "            embedding_dim=embedding_dim,\n",
    "            h_dim=decoder_h_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            num_layers=num_layers,\n",
    "            pool_every_timestep=pool_every_timestep,\n",
    "            dropout=dropout,\n",
    "            bottleneck_dim=bottleneck_dim,\n",
    "            activation=activation,\n",
    "            batch_norm=batch_norm,\n",
    "            pooling_type=pooling_type,\n",
    "            grid_size=grid_size,\n",
    "            neighborhood_size=neighborhood_size\n",
    "        )\n",
    "\n",
    "        if pooling_type == 'pool_net':\n",
    "            self.pool_net = PoolHiddenNet(\n",
    "                embedding_dim=self.embedding_dim,\n",
    "                h_dim=encoder_h_dim,\n",
    "                mlp_dim=mlp_dim,\n",
    "                bottleneck_dim=bottleneck_dim,\n",
    "                activation=activation,\n",
    "                batch_norm=batch_norm\n",
    "            )\n",
    "        elif pooling_type == 'spool':\n",
    "            self.pool_net = SocialPooling(\n",
    "                h_dim=encoder_h_dim,\n",
    "                activation=activation,\n",
    "                batch_norm=batch_norm,\n",
    "                dropout=dropout,\n",
    "                neighborhood_size=neighborhood_size,\n",
    "                grid_size=grid_size\n",
    "            )\n",
    "\n",
    "        if self.noise_dim[0] == 0:\n",
    "            self.noise_dim = None\n",
    "        else:\n",
    "            self.noise_first_dim = noise_dim[0]\n",
    "\n",
    "        # Decoder Hidden\n",
    "        if pooling_type:\n",
    "            input_dim = encoder_h_dim + bottleneck_dim\n",
    "        else:\n",
    "            input_dim = encoder_h_dim\n",
    "\n",
    "        if self.mlp_decoder_needed():\n",
    "            mlp_decoder_context_dims = [\n",
    "                input_dim, mlp_dim, decoder_h_dim - self.noise_first_dim\n",
    "            ]\n",
    "\n",
    "            self.mlp_decoder_context = make_mlp(\n",
    "                mlp_decoder_context_dims,\n",
    "                activation=activation,\n",
    "                batch_norm=batch_norm,\n",
    "                dropout=dropout\n",
    "            )\n",
    "\n",
    "    def add_noise(self, _input, seq_start_end, user_noise=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - _input: Tensor of shape (_, decoder_h_dim - noise_first_dim)\n",
    "        - seq_start_end: A list of tuples which delimit sequences within batch.\n",
    "        - user_noise: Generally used for inference when you want to see\n",
    "        relation between different types of noise and outputs.\n",
    "        Outputs:\n",
    "        - decoder_h: Tensor of shape (_, decoder_h_dim)\n",
    "        \"\"\"\n",
    "        if not self.noise_dim:\n",
    "            return _input\n",
    "\n",
    "        if self.noise_mix_type == 'global':\n",
    "            noise_shape = (seq_start_end.size(0), ) + self.noise_dim\n",
    "        else:\n",
    "            noise_shape = (_input.size(0), ) + self.noise_dim\n",
    "\n",
    "        if user_noise is not None:\n",
    "            z_decoder = user_noise\n",
    "        else:\n",
    "            z_decoder = get_noise(noise_shape, self.noise_type)\n",
    "\n",
    "        if self.noise_mix_type == 'global':\n",
    "            _list = []\n",
    "            for idx, (start, end) in enumerate(seq_start_end):\n",
    "                start = start.item()\n",
    "                end = end.item()\n",
    "                _vec = z_decoder[idx].view(1, -1)\n",
    "                _to_cat = _vec.repeat(end - start, 1)\n",
    "                _list.append(torch.cat([_input[start:end], _to_cat], dim=1))\n",
    "            decoder_h = torch.cat(_list, dim=0)\n",
    "            return decoder_h\n",
    "\n",
    "        decoder_h = torch.cat([_input, z_decoder], dim=1)\n",
    "\n",
    "        return decoder_h\n",
    "\n",
    "    def mlp_decoder_needed(self):\n",
    "        if (\n",
    "            self.noise_dim or self.pooling_type or\n",
    "            self.encoder_h_dim != self.decoder_h_dim\n",
    "        ):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def forward(self, obs_traj, obs_traj_rel, seq_start_end, user_noise=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - obs_traj: Tensor of shape (obs_len, batch, 2)\n",
    "        - obs_traj_rel: Tensor of shape (obs_len, batch, 2)\n",
    "        - seq_start_end: A list of tuples which delimit sequences within batch.\n",
    "        - user_noise: Generally used for inference when you want to see\n",
    "        relation between different types of noise and outputs.\n",
    "        Output:\n",
    "        - pred_traj_rel: Tensor of shape (self.pred_len, batch, 2)\n",
    "        \"\"\"\n",
    "        batch = obs_traj_rel.size(1)\n",
    "        # Encode seq\n",
    "        final_encoder_h = self.encoder(obs_traj_rel)\n",
    "        # Pool States\n",
    "        if self.pooling_type:\n",
    "            end_pos = obs_traj[-1, :, :]\n",
    "            pool_h = self.pool_net(final_encoder_h, seq_start_end, end_pos)\n",
    "            # Construct input hidden states for decoder\n",
    "            mlp_decoder_context_input = torch.cat(\n",
    "                [final_encoder_h.view(-1, self.encoder_h_dim), pool_h], dim=1)\n",
    "        else:\n",
    "            mlp_decoder_context_input = final_encoder_h.view(\n",
    "                -1, self.encoder_h_dim)\n",
    "\n",
    "        # Add Noise\n",
    "        if self.mlp_decoder_needed():\n",
    "            noise_input = self.mlp_decoder_context(mlp_decoder_context_input)\n",
    "        else:\n",
    "            noise_input = mlp_decoder_context_input\n",
    "        decoder_h = self.add_noise(\n",
    "            noise_input, seq_start_end, user_noise=user_noise)\n",
    "        decoder_h = torch.unsqueeze(decoder_h, 0)\n",
    "\n",
    "        decoder_c = torch.zeros(\n",
    "            self.num_layers, batch, self.decoder_h_dim\n",
    "        ).cuda()\n",
    "\n",
    "        state_tuple = (decoder_h, decoder_c)\n",
    "        last_pos = obs_traj[-1]\n",
    "        last_pos_rel = obs_traj_rel[-1]\n",
    "        # Predict Trajectory\n",
    "\n",
    "        decoder_out = self.decoder(\n",
    "            last_pos,\n",
    "            last_pos_rel,\n",
    "            state_tuple,\n",
    "            seq_start_end,\n",
    "        )\n",
    "        pred_traj_fake_rel, final_decoder_h = decoder_out\n",
    "\n",
    "        return pred_traj_fake_rel\n",
    "\n",
    "\n",
    "class TrajectoryDiscriminator(nn.Module):\n",
    "    def __init__(\n",
    "        self, obs_len, pred_len, embedding_dim=64, h_dim=64, mlp_dim=1024,\n",
    "        num_layers=1, activation='relu', batch_norm=True, dropout=0.0,\n",
    "        d_type='local'\n",
    "    ):\n",
    "        super(TrajectoryDiscriminator, self).__init__()\n",
    "\n",
    "        self.obs_len = obs_len\n",
    "        self.pred_len = pred_len\n",
    "        self.seq_len = obs_len + pred_len\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.d_type = d_type\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            embedding_dim=embedding_dim,\n",
    "            h_dim=h_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        real_classifier_dims = [h_dim, mlp_dim, 1]\n",
    "        self.real_classifier = make_mlp(\n",
    "            real_classifier_dims,\n",
    "            activation=activation,\n",
    "            batch_norm=batch_norm,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        if d_type == 'global':\n",
    "            mlp_pool_dims = [h_dim + embedding_dim, mlp_dim, h_dim]\n",
    "            self.pool_net = PoolHiddenNet(\n",
    "                embedding_dim=embedding_dim,\n",
    "                h_dim=h_dim,\n",
    "                mlp_dim=mlp_pool_dims,\n",
    "                bottleneck_dim=h_dim,\n",
    "                activation=activation,\n",
    "                batch_norm=batch_norm\n",
    "            )\n",
    "\n",
    "    def forward(self, traj, traj_rel, seq_start_end=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - traj: Tensor of shape (obs_len + pred_len, batch, 2)\n",
    "        - traj_rel: Tensor of shape (obs_len + pred_len, batch, 2)\n",
    "        - seq_start_end: A list of tuples which delimit sequences within batch\n",
    "        Output:\n",
    "        - scores: Tensor of shape (batch,) with real/fake scores\n",
    "        \"\"\"\n",
    "        final_h = self.encoder(traj_rel)\n",
    "        # Note: In case of 'global' option we are using start_pos as opposed to\n",
    "        # end_pos. The intution being that hidden state has the whole\n",
    "        # trajectory and relative postion at the start when combined with\n",
    "        # trajectory information should help in discriminative behavior.\n",
    "        if self.d_type == 'local':\n",
    "            classifier_input = final_h.squeeze()\n",
    "        else:\n",
    "            classifier_input = self.pool_net(\n",
    "                final_h.squeeze(), seq_start_end, traj[0]\n",
    "            )\n",
    "        scores = self.real_classifier(classifier_input)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
